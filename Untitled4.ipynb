{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPsTrKRvxdsrLqNJ2CCCKjM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matteograsso98/energy_forecasting/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Energy Forecaster - Neural Network Approach\n",
        "\n",
        "In this notebook, we reason about the critical importance of setting `shuffle=False` when training machine learning /AI models for time-series forecasting tasks. Time-series data inherently contains temporal dependencies, where the order of observations carries crucial information about underlying patterns and trends. Randomly shuffling the training data disrupts these temporal relationships, potentially leading to data leakage and compromised model performance. When `shuffle=True`, the model may inadvertently access future information during training, creating an unrealistic evaluation scenario that doesn't reflect real-world forecasting conditions. Key points of shuffle=True are:\n",
        "\n",
        "- Preserving temporal dependencies and seasonal patterns\n",
        "- Preventing look-ahead bias in model training\n",
        "- Ensuring realistic performance evaluation\n",
        "- Maintaining the integrity of sequential learning\n",
        "\n",
        "However, this depends on how we conceptualize the problem at hand. Setting `shuffle=False` may be appropriate for problems that are inherently time-series in nature, but than can be looked at from a different perspective. For instance, consider predicting a photovoltaic plant's energy production based on the power output of multiple plants and weather conditions at their locations. If we look at the problem as predicting the energy power given some weather conditions (and regardless of the time arrow), then setting\n",
        "`shuffle=True` can be justified. The aim of this work is to compare the two settings, and see setting  `shuffle=False` is just a misconception that can be re-thought in some circumstances.\n",
        "\n"
      ],
      "metadata": {
        "id": "k5kyEKgjFArd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras_tuner"
      ],
      "metadata": {
        "id": "WNBF0-XRRP_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, mixed_precision, callbacks, optimizers, Model\n",
        "import keras_tuner as kt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "gVdKfJs0kbf-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJWcighmFA_d",
        "outputId": "e48ec1b5-6868-4359-ac5a-39730851ff84"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "foto12 = pd.read_csv('/content/drive/MyDrive/foto12.csv')\n",
        "corrid1 = pd.read_csv('/content/drive/MyDrive/corrid1.csv')\n",
        "coper1 = pd.read_csv('/content/drive/MyDrive/coper1.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dwihZY3IIje",
        "outputId": "d3997a66-dca2-43e1-d966-d01047a82746"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Weather Data\n",
        "foto12.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8sCD8ZaKra6",
        "outputId": "82b50027-8241-4f6d-de88-c3f19656ecb3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Unnamed: 0', 'entity_type', 'created_at', 'dewPoint2mC',\n",
              "       'diffuse_rad_1h_J', 'diffuseRadW', 'direct_rad_1h_J', 'directRadW',\n",
              "       'global_rad_1h_J', 'hash_key', 'highCloudCoverP', 'lowCloudCoverP',\n",
              "       'mediumCloudCoverP', 'mslPressureHpa', 'precipitation1hMm',\n",
              "       'sfcPressureHpa', 'sort_key', 'up', 'windDirMean100m1hD',\n",
              "       'windDirMean10m1hD', 'windGusts100m1hMs', 'windGusts100mMs',\n",
              "       'windGusts10m1hMs', 'windGusts10mMs', 'windSpeedMean100m1hMs',\n",
              "       'windSpeedMean10m1hMs', 'temperature2mC', 'ghi', 'dni', 'dhi', 'ghi-1',\n",
              "       'dhi-1', 'dni-1', 'apparent_zenith', 'zenith', 'apparent_elevation',\n",
              "       'elevation', 'azimuth', 'equation_of_time', 'ideal_azimuth',\n",
              "       'max_radiation', 'overall_cloud_cover', 'previous_direct_rad',\n",
              "       'previous_hour_rad', 'previous_diffuse_rad', 'previous_max_radiation',\n",
              "       'next_direct_rad', 'next_diffuse_rad', 'next_max_radiation', 'sunrise',\n",
              "       'dusk', 'day', 'cos_h', 'sin_h', 'cos_month', 'sin_month',\n",
              "       'sin_day_of_w', 'cos_day_of_w', 'id', 'ds', 'kwh'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Energy Forecaster\n",
        "Here we create the neural network as part of a class called \"Energy Forecaster\".\n",
        "\n",
        "---\n",
        "The activation function is a parameterised sigmoid function\n",
        "\n",
        "$ \\sigma(z, \\alpha, \\beta) =  \\beta + (1 - \\beta) \\cdot \\frac{1}{1+e^{- \\alpha z}}  $\n",
        "\n",
        "---\n",
        "\n",
        "The structure of the NN defined below is:\n",
        "\n",
        "```\n",
        "raw-features (shape=(…, input_dim))\n",
        "        │\n",
        "   ┌───norm───┐\n",
        "   │ layers.Normalization()\n",
        "   │   (standardizes each feature to zero mean & unit variance, once adapted)\n",
        "   └──────────┘\n",
        "        │\n",
        "  ┌─────────────────────────┐\n",
        "  │ Hidden block (× len(hidden_units))  \n",
        "  └─────────────────────────┘\n",
        "        │\n",
        "  output_layer  ─── Dense(1)  → scalar prediction\n",
        "\n",
        "```\n",
        "where the default is set to:\n",
        "\n",
        "```\n",
        "Input → Norm\n",
        "  → Dense(64) → BatchNorm → GatedLinear → Dropout(0.3)\n",
        "  → Dense(64) → BatchNorm → GatedLinear → Dropout(0.3)\n",
        "  → Dense(1) → Output\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "9R7hGZoPHt61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable mixed precision for faster GPU training\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "def df_to_tf_dataset(X: pd.DataFrame,\n",
        "                     y: pd.Series | pd.DataFrame,\n",
        "                     batch_size: int = 32,\n",
        "                     shuffle: bool = True,\n",
        "                     buffer_size: int = 10000) -> tf.data.Dataset:\n",
        "    \"\"\"\n",
        "    Convert pandas features X and targets y into a tf.data.Dataset.\n",
        "    \"\"\"\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X.values.astype(np.float32),\n",
        "                                             y.values.astype(np.float32)))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size)\n",
        "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "class GatedLinear(layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom gated linear unit:\n",
        "      f(x) = (beta + (1 - beta) * sigmoid(alpha * x)) * x\n",
        "    with trainable scalars alpha and beta.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.alpha = None\n",
        "        self.beta = None\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.alpha = self.add_weight(\n",
        "            name=\"alpha\", shape=(1,), initializer=\"random_uniform\", trainable=True)\n",
        "        self.beta = self.add_weight(\n",
        "            name=\"beta\", shape=(1,), initializer=\"random_uniform\", trainable=True)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        return (self.beta + (1.0 - self.beta) * tf.sigmoid(self.alpha * x)) * x\n",
        "\n",
        "class EnergyForecasterNN(Model):\n",
        "    def __init__(self,\n",
        "                 input_dim: int,\n",
        "                 hidden_units: list[int] = [64, 64],\n",
        "                 dropout_rate: float = 0.3,\n",
        "                 seed: int = 42,\n",
        "                 **kwargs):\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_units = hidden_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.seed = seed\n",
        "        tf.random.set_seed(self.seed)\n",
        "\n",
        "        # feature-wise normalization\n",
        "        self.norm = layers.Normalization()\n",
        "\n",
        "        # build hidden stack\n",
        "        self.hidden_layers = []\n",
        "        for units in hidden_units:\n",
        "            self.hidden_layers.append(layers.Dense(units, use_bias=False))\n",
        "            self.hidden_layers.append(layers.BatchNormalization())\n",
        "            self.hidden_layers.append(GatedLinear())\n",
        "            self.hidden_layers.append(layers.Dropout(dropout_rate))\n",
        "\n",
        "        # output layer (no activation)\n",
        "        self.output_layer = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.norm(inputs)\n",
        "        for layer in self.hidden_layers:\n",
        "            # some layers require training flag\n",
        "            x = layer(x, training=training) if isinstance(layer, (layers.BatchNormalization, layers.Dropout)) else layer(x)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "    def get_config(self):\n",
        "        # Return everything needed to re-instantiate\n",
        "        base_config = super().get_config()\n",
        "        return {\n",
        "            **base_config,\n",
        "            \"input_dim\": self.input_dim,\n",
        "            \"hidden_units\": self.hidden_units,\n",
        "            \"dropout_rate\": self.dropout_rate,\n",
        "            \"seed\": self.seed,\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Extract our args; leave the rest (like 'dtype', 'name') to base\n",
        "        return cls(\n",
        "            input_dim=config.pop(\"input_dim\"),\n",
        "            hidden_units=config.pop(\"hidden_units\"),\n",
        "            dropout_rate=config.pop(\"dropout_rate\"),\n",
        "            seed=config.pop(\"seed\"),\n",
        "            **config,    # passes through name, dtype, etc.\n",
        "        )\n",
        "\n",
        "    def compile_and_fit(self,\n",
        "                        train_ds: tf.data.Dataset,\n",
        "                        val_ds: tf.data.Dataset | None = None,\n",
        "                        learning_rate: float = 1e-3,\n",
        "                        epochs: int = 100,\n",
        "                        early_stop_patience: int = 20,\n",
        "                        reduce_lr_patience: int = 10,\n",
        "                        ):\n",
        "        # compile\n",
        "        lr_schedule = optimizers.schedules.CosineDecay(\n",
        "            initial_learning_rate=learning_rate,\n",
        "            decay_steps=epochs * len(train_ds)  # Calculate decay steps\n",
        "        )\n",
        "        optimizer = optimizers.Adam(learning_rate=lr_schedule)\n",
        "        self.compile(optimizer=optimizer,\n",
        "                     loss='mse',\n",
        "                     metrics=['mae', 'mse'])\n",
        "\n",
        "        # callbacks\n",
        "        cbs = [\n",
        "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=early_stop_patience, restore_best_weights=True),\n",
        "            # Remove ReduceLROnPlateau as it conflicts with CosineDecay\n",
        "            # tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=reduce_lr_patience),\n",
        "            tf.keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True),\n",
        "            tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
        "        ]\n",
        "\n",
        "        # fit\n",
        "        return self.fit(train_ds,\n",
        "                        validation_data=val_ds,\n",
        "                        epochs=epochs,\n",
        "                        callbacks=cbs)\n",
        "\n",
        "def run_hyperparameter_search(\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    max_trials: int = 20,\n",
        "    executions_per_trial: int = 2,\n",
        "    directory: str = \"kt_dir\",\n",
        "    project_name: str = \"energy_forecast_hp\"\n",
        ") -> kt.Tuner:\n",
        "    \"\"\"\n",
        "    Run a Bayesian hyperparameter search over model depth, width, dropout, and learning rate.\n",
        "    Returns the Keras-Tuner Tuner object.\n",
        "    \"\"\"\n",
        "    def build_model(hp):\n",
        "        # sample hyperparams\n",
        "        n_layers = hp.Int('n_layers', 1, 4)\n",
        "        units = hp.Int('units', 16, 256, step=16)\n",
        "        dropout = hp.Float('dropout', 0.0, 0.5, step=0.1)\n",
        "        lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
        "\n",
        "        model = EnergyForecasterNN(\n",
        "            input_dim=X.shape[1],\n",
        "            hidden_units=[units] * n_layers,\n",
        "            dropout_rate=dropout\n",
        "        )\n",
        "        model.compile_and_fit(\n",
        "            train_ds=df_to_tf_dataset(X, y, batch_size=32, shuffle=True),\n",
        "            val_ds=df_to_tf_dataset(X, y, batch_size=32, shuffle=False),\n",
        "            learning_rate=lr\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    tuner = kt.BayesianOptimization(\n",
        "        build_model,\n",
        "        objective='val_loss',\n",
        "        max_trials=max_trials,\n",
        "        executions_per_trial=executions_per_trial,\n",
        "        directory=directory,\n",
        "        project_name=project_name\n",
        "    )\n",
        "\n",
        "    tuner.search(df_to_tf_dataset(X, y, batch_size=32, shuffle=True))\n",
        "    return tuner"
      ],
      "metadata": {
        "id": "KqKj6nXVj3Wk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data set"
      ],
      "metadata": {
        "id": "-CwolV4zfwAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the dataframes\n",
        "concatenated_df = pd.concat([foto12, corrid1, coper1], ignore_index=True)\n",
        "# Print some info\n",
        "#print(concatenated_df.info())"
      ],
      "metadata": {
        "id": "TRvDZSFQfva8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting into Training and Test sets"
      ],
      "metadata": {
        "id": "69F-RPWAU2Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features & raw arrays\n",
        "features = ['diffuseRadW', 'directRadW', 'temperature2mC', 'mediumCloudCoverP']\n",
        "weather_raw = concatenated_df[features].values          # shape (N, 4)\n",
        "energy_raw  = concatenated_df['kwh'].values.reshape(-1, 1)  # shape (N, 1)\n",
        "\n",
        "# Scale both X and y\n",
        "scaler_x = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "\n",
        "weather_scaled = scaler_x.fit_transform(weather_raw)\n",
        "energy_scaled  = scaler_y.fit_transform(energy_raw)\n",
        "\n",
        "# Split on the SCALED arrays (80% train / 20% test)\n",
        "split_idx = int(0.8 * len(weather_scaled))\n",
        "X_train = weather_scaled[:split_idx]\n",
        "X_val   = weather_scaled[split_idx:]\n",
        "y_train = energy_scaled[:split_idx]\n",
        "y_val   = energy_scaled[split_idx:]"
      ],
      "metadata": {
        "id": "kDVq04B32_Ez"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters optimisation"
      ],
      "metadata": {
        "id": "ug1I63oqUy9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert back to Pandas format to avoid error messages\n",
        "X_train_df = pd.DataFrame(X_train, columns=features)\n",
        "y_train_df = pd.Series(y_train.flatten(), name=\"kwh\")\n",
        "\n",
        "# Ensure the Normalization layer can adapt in float32\n",
        "mixed_precision.set_global_policy('float32')\n",
        "\n",
        "# Instantiate the Keras-Tuner with your training arrays\n",
        "tuner = run_hyperparameter_search(X_train_df, y_train_df,\n",
        "                                  max_trials=5,\n",
        "                                  executions_per_trial=2)\n",
        "\n",
        "# Tell the tuner to search—this *trains* many models internally\n",
        "tuner.search(\n",
        "    train_ds=df_to_tf_dataset(X_train_df, y_train_df, batch_size=32, shuffle=True),\n",
        "    validation_data=df_to_tf_dataset(X_val,   y_val,   batch_size=32, shuffle=False),\n",
        "    epochs=50,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Fetch results\n",
        "best_hps    = tuner.get_best_hyperparameters(1)[0]\n",
        "best_model  = tuner.get_best_models(1)[0]"
      ],
      "metadata": {
        "id": "Lq1qjR_nUtWZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build & prepare your model\n",
        "model = EnergyForecasterNN(input_dim=X_train.shape[1],\n",
        "                           hidden_units=[200, 200, 200],\n",
        "                           dropout_rate=0.3)\n",
        "\n",
        "# Create tf.data.Dataset from the scaled arrays\n",
        "train_ds = df_to_tf_dataset(\n",
        "    pd.DataFrame(X_train, columns=features),\n",
        "    pd.DataFrame(y_train, columns=['kwh']),\n",
        "    batch_size=32,\n",
        "    shuffle=True\n",
        ")\n",
        "val_ds = df_to_tf_dataset(\n",
        "    pd.DataFrame(X_val, columns=features),\n",
        "    pd.DataFrame(y_val, columns=['kwh']),\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Adapt the built-in Normalization layer (on the SCALED inputs)\n",
        "model.norm.adapt(train_ds.map(lambda x, y: x))\n",
        "mixed_precision.set_global_policy('mixed_float16') # Re-enable mixed precision for the rest of training\n",
        "\n",
        "# Train\n",
        "history = model.compile_and_fit(\n",
        "    train_ds=train_ds,\n",
        "    val_ds=val_ds,\n",
        "    epochs=300\n",
        ")\n",
        "\n",
        "# Inference & inverse-scale predictions\n",
        "pred_scaled = model.predict(val_ds)\n",
        "pred_kwh  = scaler_y.inverse_transform(pred_scaled)"
      ],
      "metadata": {
        "id": "wLOjH3AQUS5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation and training/testing\n",
        "\n",
        "We adopt the following metrics: MSE, MAE."
      ],
      "metadata": {
        "id": "zAChZGWzJOYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as mpl\n",
        "\n",
        "# Use a modern, clean font — you can install Fira Sans or use alternatives like DejaVu Sans\n",
        "mpl.rcParams.update({\n",
        "    \"font.family\": \"DejaVu Sans\",  # Change to 'Fira Sans' if installed\n",
        "    \"font.size\": 14,\n",
        "    \"axes.titlesize\": 16,\n",
        "    \"axes.labelsize\": 14,\n",
        "    \"xtick.labelsize\": 12,\n",
        "    \"ytick.labelsize\": 12,\n",
        "    \"legend.fontsize\": 12,\n",
        "    \"axes.grid\": True,\n",
        "    \"grid.alpha\": 0.3,\n",
        "    \"figure.autolayout\": True\n",
        "})\n",
        "\n",
        "# Color palette (optional customization)\n",
        "colors = [\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\", \"#d62728\"]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# Plot training & validation loss\n",
        "axes[0, 0].plot(history.history['loss'], label='Training Loss', color=colors[0], linewidth=2)\n",
        "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', color=colors[1], linewidth=2)\n",
        "axes[0, 0].set_title('Model Training Loss')\n",
        "axes[0, 0].set_xlabel('Epoch')\n",
        "axes[0, 0].set_ylabel('Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True)\n",
        "\n",
        "# Plot training & validation MAE\n",
        "axes[0, 1].plot(history.history['mae'], label='Training MAE', color=colors[0], linewidth=2)\n",
        "axes[0, 1].plot(history.history['val_mae'], label='Validation MAE', color=colors[1], linewidth=2)\n",
        "axes[0, 1].set_title('Model Training MAE')\n",
        "axes[0, 1].set_xlabel('Epoch')\n",
        "axes[0, 1].set_ylabel('MAE')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True)\n",
        "\n",
        "# Visualizing Predictions vs True Values\n",
        "axes[1, 0].plot(scaler_y.inverse_transform(y_val.reshape(-1, 1)), label='True Energy Production', color=colors[0], linewidth=1.5)\n",
        "axes[1, 0].plot(pred_kwh, label='Predicted Energy Production', color=colors[1], linestyle='--', linewidth=1.5)\n",
        "axes[1, 0].set_title('Energy Production: True vs Predicted')\n",
        "axes[1, 0].set_xlabel('Samples')\n",
        "axes[1, 0].set_ylabel('Energy Production (kWh)')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True)\n",
        "\n",
        "# Remove the empty subplot\n",
        "fig.delaxes(axes[1, 1])\n",
        "\n",
        "# Add a tight layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wgIY3Wl75AON"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model"
      ],
      "metadata": {
        "id": "JB7YrM1v4Rio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Policy is float32 when building the model\n",
        "mixed_precision.set_global_policy(\"float32\")\n",
        "\n",
        "# Re-instantiate the model with the same hyperparameters used when training\n",
        "model = EnergyForecasterNN(\n",
        "    input_dim=4,               # ← number of features you originally used\n",
        "    hidden_units=[200, 200, 200],\n",
        "    dropout_rate=0.3,\n",
        "    seed=42,\n",
        "    name=\"energy_forecaster\"   # optional, but can match original\n",
        ")\n",
        "\n",
        "# “Build” the model so its layers are created\n",
        "#    The input_shape is (batch_size, input_dim)\n",
        "model.build(input_shape=(None, 4))\n",
        "\n",
        "# Load the weights from the saved file\n",
        "model.load_weights(\"best_model_v2.h5\")\n",
        "\n",
        "# (Re-)enable mixed precision for training/inference\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")\n",
        "\n",
        "# Compile if you plan to train or evaluate further\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"mse\",\n",
        "    metrics=[\"mae\"]\n",
        ")\n",
        "\"\"\";"
      ],
      "metadata": {
        "id": "zumovCoN40J-"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}